{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Train a model with unstructured data with a minimum of 100 epochs and prove that increasing epoch decreases the loss.\n"
      ],
      "metadata": {
        "id": "L85_iIvdk-Es"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGN9K-Ties24",
        "outputId": "39efffaf-5b0f-4c3a-a695-5c1f0ea11bcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: 2.071138850788572\n",
            "Epoch 2/100, Loss: 1.823899308628619\n",
            "Epoch 3/100, Loss: 1.756481065170597\n",
            "Epoch 4/100, Loss: 1.7253228184510905\n",
            "Epoch 5/100, Loss: 1.709675774264183\n",
            "Epoch 6/100, Loss: 1.6994956201835991\n",
            "Epoch 7/100, Loss: 1.6920565080795207\n",
            "Epoch 8/100, Loss: 1.6864230001150673\n",
            "Epoch 9/100, Loss: 1.6819609958988262\n",
            "Epoch 10/100, Loss: 1.678271442842382\n",
            "Epoch 11/100, Loss: 1.6751381846379116\n",
            "Epoch 12/100, Loss: 1.6724315638989529\n",
            "Epoch 13/100, Loss: 1.6699648742228428\n",
            "Epoch 14/100, Loss: 1.66802790703804\n",
            "Epoch 15/100, Loss: 1.6660615797998555\n",
            "Epoch 16/100, Loss: 1.6643550190081728\n",
            "Epoch 17/100, Loss: 1.6628946905959643\n",
            "Epoch 18/100, Loss: 1.6615044810116164\n",
            "Epoch 19/100, Loss: 1.660192292255125\n",
            "Epoch 20/100, Loss: 1.6591070629894606\n",
            "Epoch 21/100, Loss: 1.657945504956154\n",
            "Epoch 22/100, Loss: 1.656999347814873\n",
            "Epoch 23/100, Loss: 1.655990533991409\n",
            "Epoch 24/100, Loss: 1.6550357510794456\n",
            "Epoch 25/100, Loss: 1.6541442074247006\n",
            "Epoch 26/100, Loss: 1.6533348111709807\n",
            "Epoch 27/100, Loss: 1.6525287623090277\n",
            "Epoch 28/100, Loss: 1.6518161301928034\n",
            "Epoch 29/100, Loss: 1.6512203806244743\n",
            "Epoch 30/100, Loss: 1.6505022111223704\n",
            "Epoch 31/100, Loss: 1.6498923707110034\n",
            "Epoch 32/100, Loss: 1.6491329070093281\n",
            "Epoch 33/100, Loss: 1.6486061568707546\n",
            "Epoch 34/100, Loss: 1.6480700212246828\n",
            "Epoch 35/100, Loss: 1.6474430053981381\n",
            "Epoch 36/100, Loss: 1.6469546833526352\n",
            "Epoch 37/100, Loss: 1.6464566308806445\n",
            "Epoch 38/100, Loss: 1.6458865968403278\n",
            "Epoch 39/100, Loss: 1.6456379692183374\n",
            "Epoch 40/100, Loss: 1.6449984967835676\n",
            "Epoch 41/100, Loss: 1.6445898648772412\n",
            "Epoch 42/100, Loss: 1.64420917115486\n",
            "Epoch 43/100, Loss: 1.6437039419786255\n",
            "Epoch 44/100, Loss: 1.643402051951077\n",
            "Epoch 45/100, Loss: 1.6429204054986999\n",
            "Epoch 46/100, Loss: 1.6425837348264927\n",
            "Epoch 47/100, Loss: 1.6422256941734346\n",
            "Epoch 48/100, Loss: 1.6418746972897413\n",
            "Epoch 49/100, Loss: 1.641580267120272\n",
            "Epoch 50/100, Loss: 1.6412787992816997\n",
            "Epoch 51/100, Loss: 1.6408417935310395\n",
            "Epoch 52/100, Loss: 1.640384407439974\n",
            "Epoch 53/100, Loss: 1.640150103487694\n",
            "Epoch 54/100, Loss: 1.6399762080168165\n",
            "Epoch 55/100, Loss: 1.6395903853719422\n",
            "Epoch 56/100, Loss: 1.6392641593652493\n",
            "Epoch 57/100, Loss: 1.6389155307812477\n",
            "Epoch 58/100, Loss: 1.6387201148563866\n",
            "Epoch 59/100, Loss: 1.6384445778342451\n",
            "Epoch 60/100, Loss: 1.6381568532508572\n",
            "Epoch 61/100, Loss: 1.6377495119312424\n",
            "Epoch 62/100, Loss: 1.6375803000637208\n",
            "Epoch 63/100, Loss: 1.6373494193752183\n",
            "Epoch 64/100, Loss: 1.6371008106894585\n",
            "Epoch 65/100, Loss: 1.63688391790207\n",
            "Epoch 66/100, Loss: 1.6366696970295043\n",
            "Epoch 67/100, Loss: 1.6363649956707254\n",
            "Epoch 68/100, Loss: 1.6361675080714195\n",
            "Epoch 69/100, Loss: 1.6359113570469528\n",
            "Epoch 70/100, Loss: 1.6357014887114323\n",
            "Epoch 71/100, Loss: 1.6355535191005226\n",
            "Epoch 72/100, Loss: 1.6352411995310265\n",
            "Epoch 73/100, Loss: 1.6349734629649344\n",
            "Epoch 74/100, Loss: 1.6348832098405752\n",
            "Epoch 75/100, Loss: 1.634510485221074\n",
            "Epoch 76/100, Loss: 1.634412659383786\n",
            "Epoch 77/100, Loss: 1.6341592072169664\n",
            "Epoch 78/100, Loss: 1.63394235217495\n",
            "Epoch 79/100, Loss: 1.6337334395471665\n",
            "Epoch 80/100, Loss: 1.6335128883817303\n",
            "Epoch 81/100, Loss: 1.6332035064697266\n",
            "Epoch 82/100, Loss: 1.633169280694746\n",
            "Epoch 83/100, Loss: 1.6329492109416643\n",
            "Epoch 84/100, Loss: 1.632645109569086\n",
            "Epoch 85/100, Loss: 1.6324938070545318\n",
            "Epoch 86/100, Loss: 1.6323314259555548\n",
            "Epoch 87/100, Loss: 1.6320608272227144\n",
            "Epoch 88/100, Loss: 1.6319547570082171\n",
            "Epoch 89/100, Loss: 1.6318932112091895\n",
            "Epoch 90/100, Loss: 1.631639755738061\n",
            "Epoch 91/100, Loss: 1.6313924311574843\n",
            "Epoch 92/100, Loss: 1.6312222910334053\n",
            "Epoch 93/100, Loss: 1.631071975998787\n",
            "Epoch 94/100, Loss: 1.630852049601866\n",
            "Epoch 95/100, Loss: 1.630679727109002\n",
            "Epoch 96/100, Loss: 1.630439286420086\n",
            "Epoch 97/100, Loss: 1.6303152417831583\n",
            "Epoch 98/100, Loss: 1.6301295118037062\n",
            "Epoch 99/100, Loss: 1.6299340517790333\n",
            "Epoch 100/100, Loss: 1.629775889773867\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Import the necessary libraries from PyTorch, including the core library (torch),\n",
        "modules for neural network operations (nn), optimization algorithms (optim), datasets (datasets),\n",
        "and data loading utilities (DataLoader).\n",
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define a simple neural network model\n",
        "'''\n",
        " A simple neural network model is defined using the PyTorch nn.Module class.\n",
        " The model consists of two fully connected layers (fc1 and fc2) with ReLU activation in between (relu).\n",
        " The input size is 28x28 (the size of Fashion MNIST images), and the output size is 10 (the number of classes in Fashion MNIST).\n",
        " The softmax activation is applied to convert the raw scores into probabilities.\n",
        "'''\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.softmax(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "# Load Fashion MNIST dataset\n",
        "'''\n",
        "Data transformation (transform) that converts the raw image data to PyTorch tensors and normalizes the pixel values.\n",
        "The Fashion MNIST dataset is loaded, specifying the root directory, training set (train=True), downloading if not present,\n",
        "and applying the defined transformation. A DataLoader is created to handle loading the data in batches during training.\n",
        "'''\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Instantiate the model, loss function, and optimizer\n",
        "'''\n",
        "An instance of the SimpleNN model is created, and the loss function (CrossEntropyLoss) and\n",
        "optimizer (SGD with a learning rate of 0.01) are defined.\n",
        "The optimizer will update the model parameters during training to minimize the loss.\n",
        "'''\n",
        "model = SimpleNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "'''\n",
        "Training loop, iterating over the specified number of epochs (min_epochs).\n",
        "Within each epoch, it iterates over batches of data from the train_loader.\n",
        "For each batch, it performs the following steps:\n",
        "\n",
        "1. Zeroes the gradients (optimizer.zero_grad()).\n",
        "2. Computes the model's output for the input data.\n",
        "3. Calculates the loss between the model's output and the target labels.\n",
        "4. Computes the gradients with respect to the model parameters (loss.backward()).\n",
        "5. Updates the model parameters using the optimizer (optimizer.step()).\n",
        "6. Accumulates the total loss for the epoch.\n",
        "\n",
        "At the end of each epoch, the average loss is computed and printed.\n",
        "'''\n",
        "min_epochs = 100\n",
        "for epoch in range(min_epochs):\n",
        "    total_loss = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Print average loss for the epoch\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "    print(f'Epoch {epoch + 1}/{min_epochs}, Loss: {average_loss}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, loss values decrease as number of epochs increases"
      ],
      "metadata": {
        "id": "CvQd9O1_HfQs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1.Linear regression vs Logistic regression. Model?\n"
      ],
      "metadata": {
        "id": "Oiy-Fvq0m33P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear regression and logistic regression are both popular statistical methods used in the field of machine learning, but they serve different purposes and are suited for different types of problems.\n",
        "\n",
        "**Linear Regression:**\n",
        "**Purpose:**\n",
        "\n",
        "* Linear regression is used for predicting a continuous dependent variable based on one or more independent variables.\n",
        "* It establishes a linear relationship between the input variables (independent variables) and the output variable (dependent variable).\n",
        "\n",
        "**Output:**\n",
        "\n",
        "* The output of linear regression is a continuous value. For example, predicting house prices, temperature, sales revenue, etc.\n",
        "\n",
        "**Equation:**\n",
        "\n",
        "* The equation of a simple linear regression model is typically represented as:\n",
        "y=mx+b, where\n",
        "* y is the dependent variable,\n",
        "* x is the independent variable,\n",
        "* m is the slope, and\n",
        "* b is the y-intercept.\n",
        "\n",
        "**Logistic Regression:**\n",
        "\n",
        "**Purpose:**\n",
        "\n",
        "* Logistic regression is used for binary classification problems, where the outcome variable is categorical and has only two possible classes (0 or 1).\n",
        "* It is also used for probability estimation in multi-class classification problems.\n",
        "\n",
        "**Output:**\n",
        "\n",
        "* The output of logistic regression is a probability that the given input belongs to a particular class. The logistic function (sigmoid function) is used to map the linear combination of input features to a value between 0 and 1.\n",
        "\n",
        "**Equation:**\n",
        "\n",
        "The logistic regression model uses the logistic function, and the basic equation is\n",
        "p=1/(1+e^(-(mx+b))), where\n",
        "p is the probability of the positive class.\n",
        "\n",
        "**Summary:**\n",
        "* Linear regression is used for regression problems, predicting continuous values, and has a linear relationship between variables.\n",
        "* Logistic regression is used for classification problems, predicting binary outcomes, and involves the logistic function to model probabilities."
      ],
      "metadata": {
        "id": "L1s4gjRoouNA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2. Importance of batch size while training.**\n"
      ],
      "metadata": {
        "id": "S66I8gRjk8Fm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The choice of batch size is a crucial hyperparameter in training machine learning models, especially in the context of deep learning. The batch size determines how many samples from the dataset are used in each iteration of training. Here are some key considerations for the importance of batch size:\n",
        "\n",
        "**Memory Usage:**\n",
        "\n",
        "**Smaller Batch Sizes:** Use less memory during training. This is important when dealing with large datasets that may not fit entirely into memory.\n",
        "**Larger Batch Sizes:** Utilize more memory but may lead to faster training times, especially on hardware optimized for larger batch sizes.\n",
        "\n",
        "**Computational Efficiency:**\n",
        "\n",
        "**Smaller Batch Sizes:** Require more frequent updates to the model's weights, potentially leading to a more \"noisy\" training process.\n",
        "**Larger Batch Sizes:** Benefit from vectorized operations and parallel processing, potentially accelerating training on hardware like GPUs.\n",
        "\n",
        "**Generalization:**\n",
        "\n",
        "**Smaller Batch Sizes:** The model may generalize better as it updates its weights more frequently and sees a greater variety of samples in each epoch. It can be seen as a form of regularization.\n",
        "**Larger Batch Sizes:** May converge faster, but there's a risk of overfitting, as the model might not see as much variety in each update.\n",
        "\n",
        "**Stochasticity and Noise:**\n",
        "\n",
        "* Smaller Batch Sizes: Introduce **more randomness** into the weight updates, which can help escape local minima and explore the loss landscape more thoroughly.\n",
        "\n",
        "* Larger Batch Sizes: Provide a **more stable** and deterministic training process, but might converge to suboptimal solutions if the loss landscape is complex.\n",
        "\n",
        "**Parallelism:**\n",
        "\n",
        "* Smaller Batch Sizes: Limit parallelism as each batch must be processed sequentially. This might be a concern on hardware like GPUs that are highly parallelizable.\n",
        "* Larger Batch Sizes: Allow for greater parallelism, which can significantly speed up training on hardware with parallel processing capabilities.\n",
        "Convergence and Training Dynamics:\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "* Smaller Batch Sizes: Might require **more epochs to converge** but can exhibit better convergence dynamics, especially in the early stages of training.\n",
        "* Larger Batch Sizes: May **converge more quickly** but could experience abrupt changes in the loss landscape.\n",
        "\n",
        "* The optimal batch size depends on various factors, including the dataset size, model architecture, available hardware, and the specific characteristics of the problem you're solving. It's common to experiment with different batch sizes to find the one that balances computational efficiency with model performance."
      ],
      "metadata": {
        "id": "loZrjBwZlJcq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. What is cross entropy? Accuracy vs Loss function.\n"
      ],
      "metadata": {
        "id": "-cxJVo3vo0PL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Cross Entropy:**\n",
        "\n",
        "Cross entropy is a loss function commonly used in machine learning for classification problems. It measures the performance of a classification model whose output is a probability value between 0 and 1. Cross entropy increases as the predicted probability diverges from the actual label.\n",
        "The goal during training is to minimize the cross entropy, which is equivalent to maximizing the likelihood of the true labels given the predicted probabilities.\n",
        "\n",
        "**Accuracy vs. Loss Function:**\n",
        "\n",
        "**Accuracy:** Accuracy is a metric that measures the overall correctness of your model. It is the ratio of correctly predicted instances to the total instances. While accuracy is a commonly used metric, it might not be suitable for all scenarios. For example, in imbalanced datasets, high accuracy can be achieved by simply predicting the majority class.\n",
        "\n",
        "**Loss Function:** The loss function, such as cross entropy, is used during the training phase to guide the model to make better predictions. It quantifies the difference between the predicted values and the actual values. The model adjusts its parameters to minimize this difference.\n",
        "\n",
        "**In summary:**\n",
        "\n",
        "Accuracy is a performance metric used for evaluation, especially in the context of classification. It provides a simple and intuitive measure of how well your model is doing overall.\n",
        "\n",
        "Loss functions, like cross entropy, are used during training to optimize the model. They provide a gradient that helps adjust the model parameters to improve predictions. The goal is to minimize the loss during training, which ideally leads to better accuracy on unseen data."
      ],
      "metadata": {
        "id": "UnS0ov8SvM_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4. Importance of hidden layer. Deep Neural Network.\n",
        "Practical:Train the same model of Part 1 with one hidden layer.Document the performance improvement on using this layer.**"
      ],
      "metadata": {
        "id": "O64ls7_Do2ry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The addition of a hidden layer can potentially improve the model's capacity to learn complex patterns in the data."
      ],
      "metadata": {
        "id": "4Q7LIGuKp32y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define a neural network model with one hidden layer\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)  # Input layer to hidden layer\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 10)  # Hidden layer to output layer\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.softmax(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "# Load Fashion MNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Instantiate the model, loss function, and optimizer\n",
        "model = NeuralNetwork()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "min_epochs = 100\n",
        "previous_loss = float('inf')  # Initialize with a high loss for performance improvement calculation\n",
        "for epoch in range(min_epochs):\n",
        "    total_loss = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Print average loss for the epoch\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "    print(f'Epoch {epoch + 1}/{min_epochs}, Loss: {average_loss}')\n",
        "\n",
        "    # Calculate and print performance improvement\n",
        "    improvement = previous_loss - average_loss\n",
        "    print(f'Performance Improvement: {improvement}')\n",
        "    previous_loss = average_loss\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2V8_JAIo7yR",
        "outputId": "74061484-419e-4cea-9682-0ddf4bc00170"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: 2.0802651319676624\n",
            "Performance Improvement: inf\n",
            "Epoch 2/100, Loss: 1.8011818303228186\n",
            "Performance Improvement: 0.27908330164484374\n",
            "Epoch 3/100, Loss: 1.7432845383564801\n",
            "Performance Improvement: 0.05789729196633853\n",
            "Epoch 4/100, Loss: 1.720992951886232\n",
            "Performance Improvement: 0.022291586470248204\n",
            "Epoch 5/100, Loss: 1.7072666141270065\n",
            "Performance Improvement: 0.013726337759225382\n",
            "Epoch 6/100, Loss: 1.6979074212533833\n",
            "Performance Improvement: 0.00935919287362319\n",
            "Epoch 7/100, Loss: 1.690930914268819\n",
            "Performance Improvement: 0.006976506984564423\n",
            "Epoch 8/100, Loss: 1.685619274309195\n",
            "Performance Improvement: 0.005311639959624026\n",
            "Epoch 9/100, Loss: 1.681250907846097\n",
            "Performance Improvement: 0.004368366463097795\n",
            "Epoch 10/100, Loss: 1.6775874342999733\n",
            "Performance Improvement: 0.003663473546123841\n",
            "Epoch 11/100, Loss: 1.6745298452723\n",
            "Performance Improvement: 0.0030575890276731688\n",
            "Epoch 12/100, Loss: 1.671976323066744\n",
            "Performance Improvement: 0.002553522205556158\n",
            "Epoch 13/100, Loss: 1.6697342184815072\n",
            "Performance Improvement: 0.0022421045852367705\n",
            "Epoch 14/100, Loss: 1.6676561538852863\n",
            "Performance Improvement: 0.002078064596220841\n",
            "Epoch 15/100, Loss: 1.665865649292464\n",
            "Performance Improvement: 0.0017905045928223728\n",
            "Epoch 16/100, Loss: 1.6642735891504836\n",
            "Performance Improvement: 0.0015920601419803226\n",
            "Epoch 17/100, Loss: 1.6628846855305914\n",
            "Performance Improvement: 0.00138890361989219\n",
            "Epoch 18/100, Loss: 1.6613911204754925\n",
            "Performance Improvement: 0.0014935650550989\n",
            "Epoch 19/100, Loss: 1.660225082053813\n",
            "Performance Improvement: 0.001166038421679616\n",
            "Epoch 20/100, Loss: 1.6591209549385348\n",
            "Performance Improvement: 0.0011041271152780752\n",
            "Epoch 21/100, Loss: 1.6579996615584724\n",
            "Performance Improvement: 0.0011212933800623937\n",
            "Epoch 22/100, Loss: 1.6570444367587693\n",
            "Performance Improvement: 0.0009552247997031138\n",
            "Epoch 23/100, Loss: 1.6561027486949587\n",
            "Performance Improvement: 0.000941688063810675\n",
            "Epoch 24/100, Loss: 1.6550854541091269\n",
            "Performance Improvement: 0.001017294585831774\n",
            "Epoch 25/100, Loss: 1.6542978762055256\n",
            "Performance Improvement: 0.0007875779036012887\n",
            "Epoch 26/100, Loss: 1.6535339377073845\n",
            "Performance Improvement: 0.000763938498141048\n",
            "Epoch 27/100, Loss: 1.652767135771607\n",
            "Performance Improvement: 0.0007668019357776057\n",
            "Epoch 28/100, Loss: 1.6519401985953357\n",
            "Performance Improvement: 0.0008269371762712474\n",
            "Epoch 29/100, Loss: 1.6513293646037706\n",
            "Performance Improvement: 0.0006108339915651406\n",
            "Epoch 30/100, Loss: 1.6506615992802292\n",
            "Performance Improvement: 0.0006677653235414027\n",
            "Epoch 31/100, Loss: 1.6500318482486425\n",
            "Performance Improvement: 0.0006297510315866894\n",
            "Epoch 32/100, Loss: 1.649316402894856\n",
            "Performance Improvement: 0.0007154453537865368\n",
            "Epoch 33/100, Loss: 1.6486633131498976\n",
            "Performance Improvement: 0.0006530897449583684\n",
            "Epoch 34/100, Loss: 1.6479817029001362\n",
            "Performance Improvement: 0.0006816102497613219\n",
            "Epoch 35/100, Loss: 1.6476313992858187\n",
            "Performance Improvement: 0.00035030361431753754\n",
            "Epoch 36/100, Loss: 1.647078194597891\n",
            "Performance Improvement: 0.0005532046879277441\n",
            "Epoch 37/100, Loss: 1.6465854468121965\n",
            "Performance Improvement: 0.000492747785694414\n",
            "Epoch 38/100, Loss: 1.646063543077725\n",
            "Performance Improvement: 0.0005219037344714739\n",
            "Epoch 39/100, Loss: 1.6455232567116143\n",
            "Performance Improvement: 0.0005402863661108093\n",
            "Epoch 40/100, Loss: 1.6449364156865363\n",
            "Performance Improvement: 0.0005868410250779998\n",
            "Epoch 41/100, Loss: 1.644727285482736\n",
            "Performance Improvement: 0.00020913020380031178\n",
            "Epoch 42/100, Loss: 1.644182305727432\n",
            "Performance Improvement: 0.000544979755303876\n",
            "Epoch 43/100, Loss: 1.6438005481447493\n",
            "Performance Improvement: 0.0003817575826827735\n",
            "Epoch 44/100, Loss: 1.6434743666191345\n",
            "Performance Improvement: 0.0003261815256148104\n",
            "Epoch 45/100, Loss: 1.6430588940313375\n",
            "Performance Improvement: 0.00041547258779695007\n",
            "Epoch 46/100, Loss: 1.6427199844358318\n",
            "Performance Improvement: 0.00033890959550575417\n",
            "Epoch 47/100, Loss: 1.6423329001804914\n",
            "Performance Improvement: 0.0003870842553403975\n",
            "Epoch 48/100, Loss: 1.6419242492108457\n",
            "Performance Improvement: 0.00040865096964570746\n",
            "Epoch 49/100, Loss: 1.6416317079621336\n",
            "Performance Improvement: 0.00029254124871203047\n",
            "Epoch 50/100, Loss: 1.6413360647301176\n",
            "Performance Improvement: 0.0002956432320160296\n",
            "Epoch 51/100, Loss: 1.641020974116539\n",
            "Performance Improvement: 0.0003150906135787057\n",
            "Epoch 52/100, Loss: 1.6407079708093264\n",
            "Performance Improvement: 0.00031300330721251157\n",
            "Epoch 53/100, Loss: 1.6403442157356978\n",
            "Performance Improvement: 0.00036375507362862436\n",
            "Epoch 54/100, Loss: 1.640105391616252\n",
            "Performance Improvement: 0.00023882411944575743\n",
            "Epoch 55/100, Loss: 1.6397282604469674\n",
            "Performance Improvement: 0.0003771311692846169\n",
            "Epoch 56/100, Loss: 1.6395115235975302\n",
            "Performance Improvement: 0.00021673684943723792\n",
            "Epoch 57/100, Loss: 1.639243682945715\n",
            "Performance Improvement: 0.00026784065181506556\n",
            "Epoch 58/100, Loss: 1.6388838480530517\n",
            "Performance Improvement: 0.0003598348926634465\n",
            "Epoch 59/100, Loss: 1.638618021504457\n",
            "Performance Improvement: 0.0002658265485946387\n",
            "Epoch 60/100, Loss: 1.6384061727442467\n",
            "Performance Improvement: 0.00021184876021029808\n",
            "Epoch 61/100, Loss: 1.6380833529714327\n",
            "Performance Improvement: 0.0003228197728140092\n",
            "Epoch 62/100, Loss: 1.6378101448514568\n",
            "Performance Improvement: 0.00027320811997588734\n",
            "Epoch 63/100, Loss: 1.637608503990336\n",
            "Performance Improvement: 0.0002016408611207332\n",
            "Epoch 64/100, Loss: 1.6374711137590632\n",
            "Performance Improvement: 0.00013739023127290473\n",
            "Epoch 65/100, Loss: 1.6370098382425207\n",
            "Performance Improvement: 0.0004612755165425142\n",
            "Epoch 66/100, Loss: 1.636823206059714\n",
            "Performance Improvement: 0.00018663218280656757\n",
            "Epoch 67/100, Loss: 1.6366214050667118\n",
            "Performance Improvement: 0.00020180099300226573\n",
            "Epoch 68/100, Loss: 1.6363765491859745\n",
            "Performance Improvement: 0.0002448558807373047\n",
            "Epoch 69/100, Loss: 1.6360954053874717\n",
            "Performance Improvement: 0.00028114379850285864\n",
            "Epoch 70/100, Loss: 1.6359430504506076\n",
            "Performance Improvement: 0.000152354936864052\n",
            "Epoch 71/100, Loss: 1.6357475095974612\n",
            "Performance Improvement: 0.00019554085314643643\n",
            "Epoch 72/100, Loss: 1.6354099669690325\n",
            "Performance Improvement: 0.00033754262842866645\n",
            "Epoch 73/100, Loss: 1.635280768627297\n",
            "Performance Improvement: 0.0001291983417355258\n",
            "Epoch 74/100, Loss: 1.6350892273856124\n",
            "Performance Improvement: 0.00019154124168463404\n",
            "Epoch 75/100, Loss: 1.6347329775407624\n",
            "Performance Improvement: 0.00035624984484994293\n",
            "Epoch 76/100, Loss: 1.6345698575475323\n",
            "Performance Improvement: 0.00016311999323015414\n",
            "Epoch 77/100, Loss: 1.6343772599437851\n",
            "Performance Improvement: 0.00019259760374712442\n",
            "Epoch 78/100, Loss: 1.6342166320347329\n",
            "Performance Improvement: 0.00016062790905224666\n",
            "Epoch 79/100, Loss: 1.633962519133269\n",
            "Performance Improvement: 0.00025411290146393206\n",
            "Epoch 80/100, Loss: 1.6337812976288135\n",
            "Performance Improvement: 0.00018122150445543106\n",
            "Epoch 81/100, Loss: 1.6336316981041101\n",
            "Performance Improvement: 0.0001495995247033921\n",
            "Epoch 82/100, Loss: 1.6334300179725516\n",
            "Performance Improvement: 0.0002016801315585237\n",
            "Epoch 83/100, Loss: 1.633118141053328\n",
            "Performance Improvement: 0.00031187691922363037\n",
            "Epoch 84/100, Loss: 1.632997609277778\n",
            "Performance Improvement: 0.00012053177554993688\n",
            "Epoch 85/100, Loss: 1.6327545045535448\n",
            "Performance Improvement: 0.00024310472423327845\n",
            "Epoch 86/100, Loss: 1.6325386001357138\n",
            "Performance Improvement: 0.0002159044178309255\n",
            "Epoch 87/100, Loss: 1.632374051410252\n",
            "Performance Improvement: 0.00016454872546178834\n",
            "Epoch 88/100, Loss: 1.6322007115715857\n",
            "Performance Improvement: 0.00017333983866629765\n",
            "Epoch 89/100, Loss: 1.6321020276307552\n",
            "Performance Improvement: 9.868394083056842e-05\n",
            "Epoch 90/100, Loss: 1.6318398576809654\n",
            "Performance Improvement: 0.00026216994978978114\n",
            "Epoch 91/100, Loss: 1.6317091933699812\n",
            "Performance Improvement: 0.0001306643109841854\n",
            "Epoch 92/100, Loss: 1.6315233900602946\n",
            "Performance Improvement: 0.0001858033096866496\n",
            "Epoch 93/100, Loss: 1.6311943951700287\n",
            "Performance Improvement: 0.0003289948902658324\n",
            "Epoch 94/100, Loss: 1.6311337594538609\n",
            "Performance Improvement: 6.063571616787833e-05\n",
            "Epoch 95/100, Loss: 1.6308332895165059\n",
            "Performance Improvement: 0.00030046993735499683\n",
            "Epoch 96/100, Loss: 1.6308278868447488\n",
            "Performance Improvement: 5.402671757082089e-06\n",
            "Epoch 97/100, Loss: 1.6306081147336249\n",
            "Performance Improvement: 0.00021977211112389483\n",
            "Epoch 98/100, Loss: 1.6303252082135378\n",
            "Performance Improvement: 0.0002829065200871117\n",
            "Epoch 99/100, Loss: 1.6302917564093178\n",
            "Performance Improvement: 3.345180421998606e-05\n",
            "Epoch 100/100, Loss: 1.6300442343327537\n",
            "Performance Improvement: 0.0002475220765640884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. Training a model?\n"
      ],
      "metadata": {
        "id": "u6_XBLNs6rlD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of training a neural network using PyTorch on the Fashion MNIST dataset. This example uses a basic neural network with one hidden layer.This code defines a simple neural network, loads the Fashion MNIST dataset, and trains the model using stochastic gradient descent (SGD) as the optimizer and cross-entropy loss as the loss function. The training loop iterates through the dataset for a specified number of epochs, printing the loss every 100 batches."
      ],
      "metadata": {
        "id": "AoyZb7tE6w_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define a simple neural network model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)  # Input layer to hidden layer\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 10)  # Hidden layer to output layer\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.softmax(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "# Load Fashion MNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Instantiate the model, loss function, and optimizer\n",
        "model = SimpleNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "epochs = 10  # You can adjust the number of epochs\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if batch_idx % 100 == 99:  # Print every 100 batches\n",
        "            print(f'Epoch {epoch + 1}, Batch {batch_idx + 1}, Loss: {running_loss / 100:.4f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Training finished!')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gZ5_cqL7An8",
        "outputId": "14a83b75-d6d7-4b97-b7cc-9c334c56c1c6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 100, Loss: 2.2911\n",
            "Epoch 1, Batch 200, Loss: 2.2562\n",
            "Epoch 1, Batch 300, Loss: 2.1953\n",
            "Epoch 1, Batch 400, Loss: 2.1356\n",
            "Epoch 1, Batch 500, Loss: 2.0811\n",
            "Epoch 1, Batch 600, Loss: 2.0290\n",
            "Epoch 1, Batch 700, Loss: 1.9906\n",
            "Epoch 1, Batch 800, Loss: 1.9386\n",
            "Epoch 1, Batch 900, Loss: 1.8957\n",
            "Epoch 2, Batch 100, Loss: 1.8613\n",
            "Epoch 2, Batch 200, Loss: 1.8355\n",
            "Epoch 2, Batch 300, Loss: 1.8217\n",
            "Epoch 2, Batch 400, Loss: 1.8016\n",
            "Epoch 2, Batch 500, Loss: 1.7867\n",
            "Epoch 2, Batch 600, Loss: 1.7757\n",
            "Epoch 2, Batch 700, Loss: 1.7796\n",
            "Epoch 2, Batch 800, Loss: 1.7690\n",
            "Epoch 2, Batch 900, Loss: 1.7604\n",
            "Epoch 3, Batch 100, Loss: 1.7564\n",
            "Epoch 3, Batch 200, Loss: 1.7504\n",
            "Epoch 3, Batch 300, Loss: 1.7506\n",
            "Epoch 3, Batch 400, Loss: 1.7430\n",
            "Epoch 3, Batch 500, Loss: 1.7417\n",
            "Epoch 3, Batch 600, Loss: 1.7350\n",
            "Epoch 3, Batch 700, Loss: 1.7345\n",
            "Epoch 3, Batch 800, Loss: 1.7304\n",
            "Epoch 3, Batch 900, Loss: 1.7275\n",
            "Epoch 4, Batch 100, Loss: 1.7257\n",
            "Epoch 4, Batch 200, Loss: 1.7237\n",
            "Epoch 4, Batch 300, Loss: 1.7264\n",
            "Epoch 4, Batch 400, Loss: 1.7213\n",
            "Epoch 4, Batch 500, Loss: 1.7137\n",
            "Epoch 4, Batch 600, Loss: 1.7195\n",
            "Epoch 4, Batch 700, Loss: 1.7116\n",
            "Epoch 4, Batch 800, Loss: 1.7132\n",
            "Epoch 4, Batch 900, Loss: 1.7178\n",
            "Epoch 5, Batch 100, Loss: 1.7131\n",
            "Epoch 5, Batch 200, Loss: 1.7194\n",
            "Epoch 5, Batch 300, Loss: 1.7112\n",
            "Epoch 5, Batch 400, Loss: 1.7013\n",
            "Epoch 5, Batch 500, Loss: 1.7096\n",
            "Epoch 5, Batch 600, Loss: 1.6964\n",
            "Epoch 5, Batch 700, Loss: 1.6998\n",
            "Epoch 5, Batch 800, Loss: 1.6931\n",
            "Epoch 5, Batch 900, Loss: 1.7034\n",
            "Epoch 6, Batch 100, Loss: 1.6962\n",
            "Epoch 6, Batch 200, Loss: 1.6993\n",
            "Epoch 6, Batch 300, Loss: 1.6959\n",
            "Epoch 6, Batch 400, Loss: 1.6931\n",
            "Epoch 6, Batch 500, Loss: 1.7005\n",
            "Epoch 6, Batch 600, Loss: 1.6923\n",
            "Epoch 6, Batch 700, Loss: 1.6983\n",
            "Epoch 6, Batch 800, Loss: 1.6944\n",
            "Epoch 6, Batch 900, Loss: 1.6951\n",
            "Epoch 7, Batch 100, Loss: 1.6992\n",
            "Epoch 7, Batch 200, Loss: 1.6867\n",
            "Epoch 7, Batch 300, Loss: 1.6951\n",
            "Epoch 7, Batch 400, Loss: 1.6834\n",
            "Epoch 7, Batch 500, Loss: 1.6897\n",
            "Epoch 7, Batch 600, Loss: 1.6851\n",
            "Epoch 7, Batch 700, Loss: 1.6902\n",
            "Epoch 7, Batch 800, Loss: 1.6899\n",
            "Epoch 7, Batch 900, Loss: 1.6887\n",
            "Epoch 8, Batch 100, Loss: 1.6930\n",
            "Epoch 8, Batch 200, Loss: 1.6799\n",
            "Epoch 8, Batch 300, Loss: 1.6865\n",
            "Epoch 8, Batch 400, Loss: 1.6810\n",
            "Epoch 8, Batch 500, Loss: 1.6831\n",
            "Epoch 8, Batch 600, Loss: 1.6755\n",
            "Epoch 8, Batch 700, Loss: 1.6843\n",
            "Epoch 8, Batch 800, Loss: 1.6908\n",
            "Epoch 8, Batch 900, Loss: 1.6864\n",
            "Epoch 9, Batch 100, Loss: 1.6809\n",
            "Epoch 9, Batch 200, Loss: 1.6743\n",
            "Epoch 9, Batch 300, Loss: 1.6855\n",
            "Epoch 9, Batch 400, Loss: 1.6814\n",
            "Epoch 9, Batch 500, Loss: 1.6807\n",
            "Epoch 9, Batch 600, Loss: 1.6757\n",
            "Epoch 9, Batch 700, Loss: 1.6754\n",
            "Epoch 9, Batch 800, Loss: 1.6794\n",
            "Epoch 9, Batch 900, Loss: 1.6901\n",
            "Epoch 10, Batch 100, Loss: 1.6724\n",
            "Epoch 10, Batch 200, Loss: 1.6800\n",
            "Epoch 10, Batch 300, Loss: 1.6793\n",
            "Epoch 10, Batch 400, Loss: 1.6874\n",
            "Epoch 10, Batch 500, Loss: 1.6804\n",
            "Epoch 10, Batch 600, Loss: 1.6795\n",
            "Epoch 10, Batch 700, Loss: 1.6663\n",
            "Epoch 10, Batch 800, Loss: 1.6783\n",
            "Epoch 10, Batch 900, Loss: 1.6680\n",
            "Training finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. Necessity of GPU in training. Device parameter? Practical:Compare the training times on a CPU vs. GPU\n"
      ],
      "metadata": {
        "id": "NuELkphL6oiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The necessity of a GPU (Graphics Processing Unit) in training deep learning models depends on various factors, and it's not strictly mandatory but highly beneficial in many cases. Benefits of GPU compared to CPU:\n",
        "\n",
        "**Speedup in Training Time:**\n",
        "\n",
        "* GPU Benefit: GPUs are highly parallel processors, making them well-suited for the large-scale matrix operations involved in training deep neural networks. Training times can be significantly reduced when using a GPU compared to a CPU.\n",
        "* CPU Impact: Training large models on CPUs can be computationally expensive and time-consuming.\n",
        "\n",
        "**Model Size and Complexity:**\n",
        "\n",
        "* GPU Benefit: Larger and more complex models, which have become common in deep learning, benefit more from GPU acceleration. This is because GPUs can handle the increased computational demands more efficiently than CPUs.\n",
        "* CPU Impact: Smaller models may not see as much benefit from GPU acceleration, and training on CPUs might be sufficient for such cases.\n",
        "\n",
        "**Data Size:**\n",
        "\n",
        "* GPU Benefit: Handling large datasets is more efficient on GPUs due to their parallel processing capabilities. Loading and processing batches of data can be performed in parallel, leading to faster training.\n",
        "* CPU Impact: CPUs may struggle with the parallel processing demands of large datasets, potentially leading to slower training.\n",
        "\n",
        "**Memory Requirements:**\n",
        "\n",
        "* GPU Benefit: GPUs typically have more memory than CPUs, which is important when dealing with large models and datasets.\n",
        "* CPU Impact: Memory limitations on CPUs may restrict the size of models or datasets that can be effectively handled.\n",
        "\n",
        "**Deep Learning Frameworks:**\n",
        "\n",
        "* GPU Benefit: Many deep learning frameworks, such as TensorFlow and PyTorch, are optimized for GPU usage. Operations can be automatically offloaded to the GPU, resulting in faster computations.\n",
        "* CPU Impact: While these frameworks can run on CPUs, they may not leverage the full potential of the hardware.\n",
        "\n",
        "**Device Parameter in PyTorch:**\n",
        "* In PyTorch, the device parameter is used to specify whether the computation should be performed on a CPU or a GPU. Common values for the device parameter are:\n",
        "* \"cpu\": Indicates that the computation should be performed on the CPU.\n",
        "* \"cuda\": Indicates that the computation should be performed on the GPU."
      ],
      "metadata": {
        "id": "St_Udb-c_8vI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "epochs=50\n",
        "\n",
        "# Set device to CPU\n",
        "device_cpu = torch.device(\"cpu\")\n",
        "\n",
        "# Load Fashion MNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Instantiate a simple neural network model\n",
        "model = models.resnet18()  # Using ResNet18 as an example\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = torch.nn.Linear(num_ftrs, 10)  # Modify the fully connected layer for Fashion MNIST\n",
        "\n",
        "# Training on CPU\n",
        "start_time_cpu = time.time()\n",
        "for epoch in range(epochs):\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device_cpu), target.to(device_cpu)\n",
        "        # Forward pass, backward pass, and optimization here\n",
        "end_time_cpu = time.time()\n",
        "elapsed_time_cpu = end_time_cpu - start_time_cpu\n",
        "\n",
        "# Set device to GPU\n",
        "device_gpu = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load Fashion MNIST dataset again for the GPU case\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Move the model to GPU\n",
        "model.to(device_gpu)\n",
        "\n",
        "# Training on GPU\n",
        "start_time_gpu = time.time()\n",
        "for epoch in range(epochs):\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device_gpu), target.to(device_gpu)\n",
        "        # Forward pass, backward pass, and optimization here\n",
        "end_time_gpu = time.time()\n",
        "elapsed_time_gpu = end_time_gpu - start_time_gpu\n",
        "\n",
        "print(f\"Training time on CPU: {elapsed_time_cpu} seconds\")\n",
        "print(f\"Training time on GPU: {elapsed_time_gpu} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugjzWQN5sFZT",
        "outputId": "c84b3550-a987-4fc7-aee9-808e7aa97a1f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training time on CPU: 615.3157975673676 seconds\n",
            "Training time on GPU: 611.4228255748749 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training time of GPU is less compared to CPU"
      ],
      "metadata": {
        "id": "FegFPiaW_bZS"
      }
    }
  ]
}